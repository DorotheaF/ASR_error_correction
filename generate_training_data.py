import glob
import json
import os
import random

# import ollama
import pandas as pd
from unsloth import FastLanguageModel
from datasets import Dataset

def generate_error_thinking(transcript):
    #have it check if the edited utterance and utterance match exactly, and if not explain what error in the transcription is and why it occurred
    prompt_style = """
    ### Instruction:
    You are an expert annotator teaching a new annotator how to edit a transcript generated by speech recognition software.
    You know it is critical that they learn what kind of errors speech recognition systems will add to the transcript, 
    as opposed to the errors in the original human speech. The original error and speech style in these math tutoring 
    transcripts must be preserved so that researchers can understand how students and tutors talk with each other. 
    Here are 7 lines from the transcript. You have looked at the middle utterance and provided a corrected middle utterance,
    using your knowledge about speech recognition system errors. Now, teach the new annotator by walking through your 
    thought process. Start by repeating the <middle> and <corrected_middle> utterances to check if they match exactly. 
    If they do, explain why you think the ASR system got it perfectly correct instead of introducing errors. If they 
    differ, explain what the errors are in the <middle> utterance that you corrected in the <corrected_middle> utterance,
    and explain why the ASR system made those errors. Be concise. 

    ### ASR:
    <context> {} </context>
    <middle> {} </middle> 
    <context> {} </context>
    
    ### Corrected:
    <corrected_middle> {} </corrected_middle>
    
    ### Response:
    {}
    """
    FastLanguageModel.for_inference(model)

    length_transcript = len(transcript)
    transcript["Response"] = ""
    for i in range(3, length_transcript-3):
        print(i)
        utterance = transcript.iloc[i,3] + ": " + transcript.iloc[i,5]
        edited_utterance = transcript.iloc[i,3] + ": " + transcript.iloc[i,4]
        pre_context = ((transcript.iloc[i-3,3] + ": " + transcript.iloc[i-3,5] + "\n" +
                       transcript.iloc[i-2,3] + ": " + transcript.iloc[i-2,5]) + "\n" +
                       transcript.iloc[i-1,3] + ": " + transcript.iloc[i-1,5])

        post_context = ((transcript.iloc[i+1,3] + ": " + transcript.iloc[i+1,5] + "\n" +
                       transcript.iloc[i+2,3] + ": " + transcript.iloc[i+2,5]) + "\n" +
                       transcript.iloc[i+3,3] + ": " + transcript.iloc[i+3,5])

        input_text = prompt_style.format(pre_context, utterance,post_context, edited_utterance, "")

        inputs = tokenizer([input_text], return_tensors="pt").to("cuda")

        # print(input_text)

        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=1200,
            use_cache=True,
        )
        response = tokenizer.batch_decode(outputs)
        print(response[0].split("### Response:")[1])


        transcript.iloc[i,11] = response[0].split("### Response:")[1]

        # Print the response
        # with open("test.txt", 'w') as file:
        #     file.write(response['message']['content'])

        # print(response['message']['content'])`
    return transcript

def format_as_dataset(transcript):

    prompt_style = """
    ### Instruction:
    This is an excerpt from a discussion between tutors and students in a zoom classroom. The transcript was automatically generated by a speech recognition system. This system introduced errors into the transcription. This error significantly impacts downstream tasks, reducing accuracy and increasing bias. It is critical to KEEP mistakes that were originally made by the student or tutor, but CORRECT errors created by the speech recognition system. Here are 7 lines from the transcript. Look at the middle utterance and 1) print the utterance, 2) indicate if the utterance has one or more errors introduced by the speech recognition system, and 3) if there are introduced errors, what is the corrected sentence. Several lines of context are provided to help decide if the utterance was transcribed accurately, do not analyse errors in those lines. 

    ### ASR:
    <context> {} </context>
    <middle> {} </middle> 
    <context> {} </context>

    ### Response:
    <think>{} </think>
    1) {}
    2) {}
    3) {}
    """

    # Okay, so I have to identify errors in the speech recognition transcript that wouldn't be there in the
    print(transcript.columns)

    length_transcript = len(transcript)
    transcript["training_prompt"] = ""
    failures = 0
    for i in range(3, length_transcript - 3):
        print(i)
        utterance = transcript.iloc[i, 3] + ": " + transcript.iloc[i, 5]
        edited_utterance = transcript.iloc[i, 3] + ": " + transcript.iloc[i, 4]
        pre_context = ((transcript.iloc[i - 3, 3] + ": " + transcript.iloc[i - 3, 5] + "\n" +
                        transcript.iloc[i - 2, 3] + ": " + transcript.iloc[i - 2, 5]) + "\n" +
                       transcript.iloc[i - 1, 3] + ": " + transcript.iloc[i - 1, 5])

        post_context = ((transcript.iloc[i + 1, 3] + ": " + transcript.iloc[i + 1, 5] + "\n" +
                         transcript.iloc[i + 2, 3] + ": " + transcript.iloc[i + 2, 5]) + "\n" +
                        transcript.iloc[i + 3, 3] + ": " + transcript.iloc[i + 3, 5])

        try:
            reasoning = transcript.iloc[i, 12].split("</think>")[-1].replace(EOS_token, "")

            if utterance == edited_utterance:
                errors = "There are no errors introduced by the ASR system"
                edited_utterance = "N/A"
            else:
                errors = "There are likely errors introduced by the ASR system"

            training_text = prompt_style.format(pre_context, utterance, post_context, reasoning, utterance, errors, edited_utterance) + EOS_token

            transcript.iloc[i,13] = training_text #TODO 12
        except:
            print("no think token")
            failures +=1
    print(failures)
    return transcript

def format_as_dataset_no_think(transcript):

    prompt_style = """
    ### Instruction:
    This is an excerpt from a discussion between tutors and students in a zoom classroom. The transcript was automatically generated by a speech recognition system. This system introduced errors into the transcription. This error significantly impacts downstream tasks, reducing accuracy and increasing bias. It is critical to KEEP mistakes that were originally made by the student or tutor, but CORRECT errors created by the speech recognition system. Here are 7 lines from the transcript. Look at the middle utterance and 1) print the utterance, 2) indicate if the utterance has one or more errors introduced by the speech recognition system, and 3) if there are introduced errors, what is the corrected sentence. Several lines of context are provided to help decide if the utterance was transcribed accurately, do not analyse errors in those lines. 

    ### ASR:
    <context> {} </context>
    <middle> {} </middle> 
    <context> {} </context>

    ### Response:
    1) {}
    2) {}
    3) {}
    """

    print(transcript.columns)

    length_transcript = len(transcript)
    transcript["training_prompt"] = ""
    for i in range(3, length_transcript - 3):
        print(i)
        utterance = transcript.iloc[i, 3] + ": " + transcript.iloc[i, 5]
        edited_utterance = transcript.iloc[i, 3] + ": " + transcript.iloc[i, 4]
        pre_context = ((transcript.iloc[i - 3, 3] + ": " + transcript.iloc[i - 3, 5] + "\n" +
                        transcript.iloc[i - 2, 3] + ": " + transcript.iloc[i - 2, 5]) + "\n" +
                       transcript.iloc[i - 1, 3] + ": " + transcript.iloc[i - 1, 5])

        post_context = ((transcript.iloc[i + 1, 3] + ": " + transcript.iloc[i + 1, 5] + "\n" +
                         transcript.iloc[i + 2, 3] + ": " + transcript.iloc[i + 2, 5]) + "\n" +
                        transcript.iloc[i + 3, 3] + ": " + transcript.iloc[i + 3, 5])



        if utterance == edited_utterance:
            errors = "There are no errors introduced by the ASR system"
            edited_utterance = "N/A"
        else:
            errors = "There are likely errors introduced by the ASR system"

        training_text = prompt_style.format(pre_context, utterance, post_context, utterance, errors, edited_utterance) + EOS_token

        transcript.iloc[i,11] = training_text #TODO 12
    return transcript

def cycle_through_transcripts(location, save_path, generate):
    print(location)
    transcripts = glob.glob(location + "*.xlsx")
    print(transcripts)

    if generate == True:
        for file in transcripts:
            file_name = file.rsplit("/",1)[1].split(".xlsx")[0]
            print(file)
            transcript = pd.read_excel(file)
            print(len(transcript))
            transcript = transcript.dropna(how="any")
            transcript['ASR'] = transcript['ASR'].astype(str)
            transcript['transcript'] = transcript['transcript'].astype(str)
            print(len(transcript))
            transcript_thinking = generate_error_thinking(transcript)
            transcript_thinking.to_excel(save_path + file_name + "_generated_reasoning.xlsx", index=False)
            # transcript_thinking = pd.read_excel("data/reasoning_generated/test_file_prepped.xlsx")
            transcript_formatted = format_as_dataset(transcript_thinking)
            transcript_formatted = transcript_formatted[transcript_formatted['training_prompt']!=""]
            prompt_list = transcript_formatted["training_prompt"].values.tolist()
            prompts = {}
            prompts["text"] = prompt_list
            with open(save_path + file_name + "_generated_reasoning_train.json", "w") as outfile:
                outfile.write(json.dumps(prompts))

    else:
        for file in transcripts:
            file_name = file.rsplit("/",1)[1].split(".xlsx")[0]
            print(file)
            transcript = pd.read_excel(file)
            transcript = transcript.dropna(how="any")
            transcript['ASR'] = transcript['ASR'].astype(str)
            transcript['transcript'] = transcript['transcript'].astype(str)
            transcript_formatted = format_as_dataset_no_think(transcript)
            transcript_formatted = transcript_formatted[transcript_formatted['training_prompt']!=""]
            prompt_list = transcript_formatted["training_prompt"].values.tolist()
            prompts = {}
            prompts["text"] = prompt_list
            with open(save_path + file_name + "_train.json", "w") as outfile:
                outfile.write(json.dumps(prompts))



def make_mega_dataset():
    prompts_reasoning = glob.glob("data/reasoning_generated/" + "*.json")
    prompts_plain = glob.glob("data/transcripts_no_reasoning/" + "*.json")

    reasoning_data = []

    for transcript in prompts_reasoning:
        with open(transcript, "r") as file:
            prompt_list = json.load(file)
        reasoning_data = reasoning_data + prompt_list["text"]

    plain_data = []

    for file in prompts_plain:
        with open(file, "r") as reader:
            prompt_list = json.load(reader)
        plain_data = plain_data + prompt_list["text"]

    len_reas = len(reasoning_data)
    reasoning_train = reasoning_data[0:(len_reas*4/5)]
    reasoning_test = reasoning_data[(len_reas*4/5):len_reas]

    print("reasoning train examples: " + str(len(reasoning_train)) )
    print("reasoning test examples: " + str(len(reasoning_test)))

    len_plain = len(plain_data)
    plain_train = plain_data[0:(len_plain * 4 / 5)]
    plain_test = plain_data[(len_plain * 4 / 5):len_plain]

    print("plain train examples: " + str(len(plain_train)))
    print("plain test examples: " + str(len(plain_test)))

    train = {}
    train["text"] = random.shuffle(reasoning_train + plain_train)

    test = {}
    test["text"] = random.shuffle(reasoning_test + plain_test)
    with open("data/train_files/mix_reasoning_plain_all_train.json", "w") as outfile:
        outfile.write(json.dumps(train))

    with open("data/train_files/mix_reasoning_plain_all_test.json", "w") as outfile:
        outfile.write(json.dumps(test))



print("started")

# location = "/mnt/c/Users/Dorot/Emotive Computing Dropbox/Dorothea French/ASR_error_correction/data/transcripts_test/"
# save_path = "/mnt/c/Users/Dorot/Emotive Computing Dropbox/Dorothea French/ASR_error_correction/data/reasoning_generated/"

location = "data/transcripts/"
save_path = "data/reasoning_generated/"
#
# location = "data/transcripts_no_reasoning/"
# save_path = "data/transcripts_no_reasoning/"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit", #TODO
    # model_name = "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit",
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    token = os.getenv("HUGGING_FACE"), # use one if using gated models #TODO
)
EOS_token = tokenizer.eos_token

print("loaded model")

cycle_through_transcripts(location, save_path, True)

