import glob
import json

import ollama
import pandas as pd
import unsloth

model, tokenizer = unsloth.FastModel.from_pretrained(
    model_name = "DeepSeek-R1-Distill-Llama-70B-bnb-4bit", #TODO
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    token = "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", # use one if using gated models #TODO
)

def seperate_transcripts(transcript):
    train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context. 
    Write a response that appropriately completes the request. 
    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

    ### Instruction:
    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. 
    Please answer the following medical question. 

    ### ASR:
    {}

    ### Response:
    <think>
    {}
    </think>
    1) {}
    2) {}
    3) {}
    4) {}"""

    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN

    def formatting_prompts_func(examples):
        inputs = examples["Question"]
        cots = examples["Complex_CoT"]
        outputs = examples["Response"]
        texts = []
        for input, cot, output in zip(inputs, cots, outputs):
            text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
            texts.append(text)
        return {
            "text": texts,
        }
def generate_error_thinking(transcript):
    prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context. 
    Write a response that appropriately completes the request. 
    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

    ### Instruction:
    You are an expert annotator editing machine generated (ASR) transcripts of classroom discourse. You know it is critical to
    correct any speech errors generated by the ASR system but preserve any errors in the original speech. 
    You have been given the ASR utterance with context, and have edited the utterance to reduce ASR error. Reason 
    through the changes you have made. 

    ### ASR:
    <context>{} </context>
    <utterance>{}</utterance>
    <context>{}</context>

    ### Response:
    <edited_utterance>{}</edited_utterance>
    <think>
    {}
    </think>"""
    unsloth.FastLanguageModel.for_inference(model)

    length_transcript = len(transcript)
    for i in range(3, length_transcript-3):
        utterance = transcript.iloc[i,3] + ": " + transcript.iloc[i,5]
        edited_utterance = transcript.iloc[i,3] + ": " + transcript.iloc[i,4]
        pre_context = ((transcript.iloc[i-3,3] + ": " + transcript.iloc[i-3,5] + "\n" +
                       transcript.iloc[i-2,3] + ": " + transcript.iloc[i-2,5]) + "\n" +
                       transcript.iloc[i-1,3] + ": " + transcript.iloc[i-1,5])

        post_context = ((transcript.iloc[i+1,3] + ": " + transcript.iloc[i+1,5] + "\n" +
                       transcript.iloc[i+2,3] + ": " + transcript.iloc[i+2,5]) + "\n" +
                       transcript.iloc[i+3,3] + ": " + transcript.iloc[i+3,5])

        inputs = tokenizer([prompt_style.format(pre_context, utterance,post_context, edited_utterance, "")], return_tensors="pt").to("cuda")

        print(inputs)

        # outputs = model.generate(
        #     input_ids=inputs.input_ids,
        #     attention_mask=inputs.attention_mask,
        #     max_new_tokens=1200,
        #     use_cache=True,
        # )
        # response = tokenizer.batch_decode(outputs)
        # print(response[0].split("### Response:")[1])


        # transcript.iloc[i,11] = response['message']['content']

        # Print the response
        # with open("test.txt", 'w') as file:
        #     file.write(response['message']['content'])

        # print(response['message']['content'])

    # transcript.to_excel("test_file.xlsx")

def cycle_through_transcripts(location, save_path):
    transcripts= glob.glob(location + "*.xlsx")
    print(transcripts)

    prompt_list = {}

    for file in transcripts:
        transcript = pd.read_excel(file)
        # transcript = seperate_transcripts(transcript)
        generate_error_thinking()
        # prompt_list = prompt_list + transcript

    # with open (save_path + "ASR_human_training_all.json", "w") as outfile:
    #     outfile.write(json.dumps(prompt_list))



location = "data/raw/"
save_path = "/data/transcript_segmented_only/"

cycle_through_transcripts(location, save_path)

# generate_error_thinking()


