import glob
import json
import os
# import ollama
import pandas as pd
from unsloth import FastLanguageModel
from datasets import Dataset

def seperate_transcripts(transcript):
    train_prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context. 
    Write a response that appropriately completes the request. 
    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

    ### Instruction:
    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. 
    Please answer the following medical question. 

    ### ASR:
    {}

    ### Response:
    <think>
    {}
    </think>
    1) {}
    2) {}
    3) {}
    4) {}"""

    EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN

    def formatting_prompts_func(examples):
        inputs = examples["Question"]
        cots = examples["Complex_CoT"]
        outputs = examples["Response"]
        texts = []
        for input, cot, output in zip(inputs, cots, outputs):
            text = train_prompt_style.format(input, cot, output) + EOS_TOKEN
            texts.append(text)
        return {
            "text": texts,
        }
def generate_error_thinking(transcript):
    #have it check if the edited utterance and utterance match exactly, and if not explain what error in the transcription is and why it occurred
    prompt_style = """
    ### Instruction:
    You are an expert annotator teaching a new annotator how to edit a transcript generated by speech recognition software.
    You know it is critical that they learn what kind of errors speech recognition systems will add to the transcript, 
    as opposed to the errors in the original human speech. The original error and speech style in these math tutoring 
    transcripts must be preserved so that researchers can understand how students and tutors talk with each other. 
    Here are 7 lines from the transcript. You have looked at the middle utterance and provided a corrected middle utterance,
    using your knowledge about speech recognition system errors. Now, teach the new annotator by walking through your 
    thought process. Start by repeating the <middle> and <corrected_middle> utterances to check if they match exactly. 
    If they do, explain why you think the ASR system got it perfectly correct instead of introducing errors. If they 
    differ, explain what the errors are in the <middle> utterance that you corrected in the <corrected_middle> utterance,
    and explain why the ASR system made those errors. 

    ### ASR:
    <context> {} </context>
    <middle> {} </middle> 
    <context> {} </context>
    
    ### Corrected:
    <corrected_middle> {} </corrected_middle>
    
    ### Response:
    {}
    """
    FastLanguageModel.for_inference(model)

    length_transcript = len(transcript)
    transcript["Response"] = ""
    for i in range(3, length_transcript-3):
        print(i)
        utterance = transcript.iloc[i,3] + ": " + transcript.iloc[i,5]
        edited_utterance = transcript.iloc[i,3] + ": " + transcript.iloc[i,4]
        pre_context = ((transcript.iloc[i-3,3] + ": " + transcript.iloc[i-3,5] + "\n" +
                       transcript.iloc[i-2,3] + ": " + transcript.iloc[i-2,5]) + "\n" +
                       transcript.iloc[i-1,3] + ": " + transcript.iloc[i-1,5])

        post_context = ((transcript.iloc[i+1,3] + ": " + transcript.iloc[i+1,5] + "\n" +
                       transcript.iloc[i+2,3] + ": " + transcript.iloc[i+2,5]) + "\n" +
                       transcript.iloc[i+3,3] + ": " + transcript.iloc[i+3,5])

        input_text = prompt_style.format(pre_context, utterance,post_context, edited_utterance, "")

        inputs = tokenizer([input_text], return_tensors="pt").to("cuda")

        # print(input_text)

        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=1200,
            use_cache=True,
        )
        response = tokenizer.batch_decode(outputs)
        print(response[0].split("### Response:")[1])


        transcript.iloc[i,11] = response[0].split("### Response:")[1]

        # Print the response
        # with open("test.txt", 'w') as file:
        #     file.write(response['message']['content'])

        # print(response['message']['content'])`
    return transcript

def format_as_dataset(transcript, save_path):

    prompt_style = """
    ### Instruction:
    This is an excerpt from a discussion between tutors and students in a zoom classroom. The transcript was automatically generated by a speech recognition system. This system introduced errors into the transcription. This error significantly impacts downstream tasks, reducing accuracy and increasing bias. It is critical to KEEP mistakes that were originally made by the student or tutor, but CORRECT errors created by the speech recognition system. Here are 7 lines from the transcript. Look at the middle utterance and 1) print the utterance, 2) indicate if the utterance has one or more errors introduced by the speech recognition system, and 3) if there are introduced errors, what is the corrected sentence. Several lines of context are provided to help decide if the utterance was transcribed accurately, do not analyse errors in those lines. 

    ### ASR:
    <context> {} </context>
    <middle> {} </middle> 
    <context> {} </context>

    ### Response:
    <think>{} </think>
    1) {}
    2) {}
    3) {}
    """

    # Okay, so I have to identify errors in the speech recognition transcript that wouldn't be there in the
    print(transcript.columns)

    length_transcript = len(transcript)
    transcript["training_prompt"] = ""
    failures = 0
    for i in range(3, length_transcript - 3):
        print(i)
        utterance = transcript.iloc[i, 3] + ": " + transcript.iloc[i, 5]
        edited_utterance = transcript.iloc[i, 3] + ": " + transcript.iloc[i, 4]
        pre_context = ((transcript.iloc[i - 3, 3] + ": " + transcript.iloc[i - 3, 5] + "\n" +
                        transcript.iloc[i - 2, 3] + ": " + transcript.iloc[i - 2, 5]) + "\n" +
                       transcript.iloc[i - 1, 3] + ": " + transcript.iloc[i - 1, 5])

        post_context = ((transcript.iloc[i + 1, 3] + ": " + transcript.iloc[i + 1, 5] + "\n" +
                         transcript.iloc[i + 2, 3] + ": " + transcript.iloc[i + 2, 5]) + "\n" +
                        transcript.iloc[i + 3, 3] + ": " + transcript.iloc[i + 3, 5])

        try:
            reasoning = transcript.iloc[i, 12].split("</think>")[-1].replace(EOS_token, "")

            if utterance == edited_utterance:
                errors = "There are no errors introduced by the ASR system"
                edited_utterance = "N/A"
            else:
                errors = "There are likely errors introduced by the ASR system"

            training_text = prompt_style.format(pre_context, utterance, post_context, reasoning, utterance, errors, edited_utterance) + EOS_token

            transcript.iloc[i,13] = training_text #TODO 12
        except:
            print("no think token")
            failures +=1
    print(failures)
    return transcript

def cycle_through_transcripts(location, save_path):
    print(location)
    transcripts = glob.glob(location + "*.xlsx")
    print(transcripts)

    prompt_list = []

    for file in transcripts:
        file_name = file.rsplit("/",1)[1].split(".xlsx")[0]
        print(file)
        transcript = pd.read_excel(file)
        transcript = transcript.dropna(how="any")
        transcript_thinking = generate_error_thinking(transcript)
        transcript_thinking.to_excel(save_path + file_name + "_generated_reasoning.xlsx", index=False)
        # transcript_thinking = pd.read_excel("data/reasoning_generated/test_file_prepped.xlsx")
        transcript_formatted = format_as_dataset(transcript_thinking, save_path)
        transcript_formatted = transcript_formatted[transcript_formatted['training_prompt']!=""]
        prompt_list = transcript_formatted["training_prompt"].values.tolist()
        prompts = {}
        prompts["text"] = prompt_list
        with open(save_path + file_name + "_generated_reasoning_train.json", "w") as outfile:
            outfile.write(json.dumps(prompts))


    print(prompt_list)






print("started")

# location = "/mnt/c/Users/Dorot/Emotive Computing Dropbox/Dorothea French/ASR_error_correction/data/transcripts_test/"
# save_path = "/mnt/c/Users/Dorot/Emotive Computing Dropbox/Dorothea French/ASR_error_correction/data/reasoning_generated/"

location = "data/transcripts/"
save_path = "data/reasoning_generated/"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit", #TODO
    # model_name = "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit",
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    token = os.getenv("HUGGING_FACE"), # use one if using gated models #TODO
)
EOS_token = tokenizer.eos_token

print("loaded model")

cycle_through_transcripts(location, save_path)

